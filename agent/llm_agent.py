"""
LLM-powered Quality Agent using Claude via LangChain.

This module implements the V2 decision layer, replacing hard-coded
thresholds with intelligent LLM reasoning.
"""

import logging
import os
from typing import Any

from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from langgraph.prebuilt import create_react_agent

from agent.tools import get_all_tools, set_context
from agent.quality_agent import QualityReport, Issue, Decision, Severity
from rag.vector_store import VectorRulesStore

logger = logging.getLogger(__name__)

# Load environment variables from .env file
load_dotenv()


SYSTEM_PROMPT = """You are a Data Quality Analyst Agent. Your role is to analyze datasets and produce quality assessments based on documented quality rules.

## Your Analysis Process

1. **Gather Facts**: Use the available tools to collect data quality metrics:
   - Start with get_dataset_overview() to understand the dataset
   - Check get_missing_values_stats() for completeness issues
   - Check get_outlier_stats() for anomalies
   - Check get_negative_values_stats() for impossible values

2. **Consult Rules**: Use search_quality_rules() to find relevant rules for any issues detected.

3. **Reason and Decide**: Based on facts and rules, determine the appropriate decision:
   - **ACCEPT**: Dataset passes all quality checks with no significant issues
   - **WARNING**: Dataset has issues requiring attention but can be used with caution
   - **REJECT**: Dataset has critical issues that make it unsuitable for use

## Decision Criteria

- Empty dataset (0 rows) → REJECT (DQ-02)
- Missing values > 40% in critical columns → REJECT (DQ-01)
- Missing values 20-40% → WARNING (DQ-01)
- Outliers > 5% in numeric columns → WARNING (DQ-05)
- Negative values in columns that should be positive → WARNING (DQ-04)

## Output Format

After your analysis, provide your response in this EXACT format:

DECISION: [ACCEPT|WARNING|REJECT]

SUMMARY: [One sentence summary of the dataset quality]

ISSUES:
- [Issue 1 with rule reference, e.g., "Column 'age' has 25% missing values (DQ-01)"]
- [Issue 2 with rule reference]
...
(Leave empty if no issues)

REASONING:
[2-3 sentences explaining your reasoning process and how you applied the rules]

## Important Guidelines

- Always check ALL aspects before deciding: missing values, outliers, negative values
- Reference specific rule IDs (DQ-01, DQ-02, etc.) for each issue
- Be precise with percentages and thresholds
- If the dataset is empty (0 rows), immediately REJECT with reference to DQ-02
- Consider column criticality when assessing severity"""


class LLMQualityAgent:
    """
    LLM-powered quality agent using Claude for intelligent reasoning.

    This agent maintains the three-layer architecture:
    - Facts: Provided by DataProfiler (accessed via tools)
    - Knowledge: Retrieved from VectorRulesStore via semantic search
    - Decision: Generated by Claude with tool-based reasoning
    """

    DEFAULT_MODEL = "claude-sonnet-4-20250514"
    TEMPERATURE = 0.0  # Deterministic for reproducibility

    def __init__(
        self,
        vector_store: VectorRulesStore,
        model_name: str | None = None,
    ):
        """
        Initialize the LLM agent.

        Args:
            vector_store: Initialized VectorRulesStore for rule retrieval.
            model_name: Optional Claude model name override.

        Raises:
            ValueError: If ANTHROPIC_API_KEY is not set.
        """
        self.vector_store = vector_store
        self.model_name = model_name or os.getenv("CLAUDE_MODEL") or self.DEFAULT_MODEL

        # Verify API key
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError(
                "ANTHROPIC_API_KEY environment variable not set. "
                "Please create a .env file with your API key. "
                "See .env.example for template."
            )

        logger.info(f"Initializing LLMQualityAgent with model: {self.model_name}")

        # Initialize Claude via LangChain
        self.llm = ChatAnthropic(
            model=self.model_name,
            temperature=self.TEMPERATURE,
            api_key=api_key,
            max_tokens=4096,
        )

        # Get tools
        self.tools = get_all_tools()

        # Create the agent using langgraph's create_react_agent
        self.agent = create_react_agent(
            model=self.llm,
            tools=self.tools,
            prompt=SYSTEM_PROMPT,
        )

        logger.info("LLMQualityAgent initialized successfully")

    def analyze(self, profile: dict[str, Any]) -> QualityReport:
        """
        Analyze a data profile and produce a quality report.

        Args:
            profile: Data profile from DataProfiler.

        Returns:
            QualityReport with decision, summary, and issues.
        """
        logger.info("=" * 60)
        logger.info("Starting LLM-powered quality analysis...")
        logger.info("=" * 60)

        # Set context for tools
        set_context(profile, self.vector_store)

        # Build the analysis request
        row_count = profile["basic_stats"]["row_count"]
        col_count = profile["basic_stats"]["column_count"]

        input_text = f"""Analyze this dataset for quality issues and provide your assessment.

Dataset: {profile.get('file_path', 'Unknown')}
Size: {row_count:,} rows x {col_count} columns

Please use the available tools to gather statistics, consult relevant quality rules, and then provide your final assessment in the required format."""

        # Run the agent
        try:
            logger.info("Invoking LLM agent...")
            # langgraph uses messages format
            messages = [{"role": "user", "content": input_text}]
            result = self.agent.invoke({"messages": messages})

            # Extract the final response from the agent
            output_messages = result.get("messages", [])
            output = ""
            if output_messages:
                # Get the last AI message content
                for msg in reversed(output_messages):
                    if hasattr(msg, 'content') and msg.type == "ai":
                        output = msg.content
                        break

            logger.info("Agent completed. Parsing response...")

            # Parse the LLM response into a QualityReport
            return self._parse_response(output, profile)

        except Exception as e:
            logger.error(f"Agent execution failed: {e}")
            # Fallback to a safe REJECT
            return QualityReport(
                decision=Decision.REJECT.value,
                summary=f"Analysis failed due to error: {str(e)}",
                issues=[Issue(
                    type="Analysis Error",
                    severity=Severity.CRITICAL.value,
                    rule_reference="N/A",
                    explanation=f"LLM analysis failed: {str(e)}"
                )],
                stats={
                    "row_count": profile["basic_stats"]["row_count"],
                    "column_count": profile["basic_stats"]["column_count"],
                    "issues_count": 1,
                }
            )

    def _parse_response(
        self,
        response: str,
        profile: dict[str, Any]
    ) -> QualityReport:
        """
        Parse LLM response into a QualityReport.

        Args:
            response: Raw LLM output text.
            profile: Original data profile.

        Returns:
            Structured QualityReport.
        """
        logger.debug(f"Parsing response:\n{response[:500]}...")

        # Extract decision
        decision = Decision.WARNING.value  # Default
        response_upper = response.upper()

        if "DECISION: ACCEPT" in response_upper or "DECISION:ACCEPT" in response_upper:
            decision = Decision.ACCEPT.value
        elif "DECISION: REJECT" in response_upper or "DECISION:REJECT" in response_upper:
            decision = Decision.REJECT.value
        elif "DECISION: WARNING" in response_upper or "DECISION:WARNING" in response_upper:
            decision = Decision.WARNING.value

        logger.info(f"Extracted decision: {decision}")

        # Extract summary
        summary = "Quality analysis completed."
        if "SUMMARY:" in response:
            try:
                summary_start = response.find("SUMMARY:") + len("SUMMARY:")
                summary_end = response.find("\n", summary_start)
                if summary_end > summary_start:
                    summary = response[summary_start:summary_end].strip()
            except Exception:
                pass

        # Extract issues
        issues = []
        if "ISSUES:" in response:
            try:
                issues_start = response.find("ISSUES:")
                # Find end of issues section
                reasoning_pos = response.find("REASONING:", issues_start)
                issues_end = reasoning_pos if reasoning_pos > issues_start else len(response)

                issues_text = response[issues_start:issues_end]

                for line in issues_text.split("\n"):
                    line = line.strip()
                    if line.startswith("-") and len(line) > 2:
                        issue_text = line[1:].strip()

                        # Skip empty or trivial lines
                        if not issue_text or issue_text.lower() in ["none", "n/a", ""]:
                            continue

                        # Extract rule reference
                        rule_ref = "N/A"
                        for dq in ["DQ-01", "DQ-02", "DQ-03", "DQ-04", "DQ-05",
                                   "DQ-06", "DQ-07", "DQ-08", "DQ-09", "DQ-10"]:
                            if dq in issue_text:
                                rule_ref = dq
                                break

                        # Determine severity based on decision and keywords
                        severity = Severity.MEDIUM.value
                        if decision == Decision.REJECT.value:
                            severity = Severity.CRITICAL.value
                        elif "critical" in issue_text.lower() or "> 40%" in issue_text:
                            severity = Severity.HIGH.value

                        # Extract column name if present
                        column = None
                        if "column" in issue_text.lower() or "'" in issue_text:
                            import re
                            col_match = re.search(r"['\"]([^'\"]+)['\"]", issue_text)
                            if col_match:
                                column = col_match.group(1)

                        issues.append(Issue(
                            type=self._extract_issue_type(issue_text),
                            severity=severity,
                            rule_reference=rule_ref,
                            explanation=issue_text,
                            column=column,
                        ))
            except Exception as e:
                logger.warning(f"Error parsing issues: {e}")

        logger.info(f"Extracted {len(issues)} issues")

        return QualityReport(
            decision=decision,
            summary=summary,
            issues=issues,
            stats={
                "row_count": profile["basic_stats"]["row_count"],
                "column_count": profile["basic_stats"]["column_count"],
                "issues_count": len(issues),
            }
        )

    def _extract_issue_type(self, issue_text: str) -> str:
        """
        Extract issue type from issue text.

        Args:
            issue_text: Raw issue description.

        Returns:
            Categorized issue type.
        """
        issue_lower = issue_text.lower()

        if "missing" in issue_lower:
            return "Missing values"
        elif "outlier" in issue_lower:
            return "Outliers"
        elif "negative" in issue_lower:
            return "Negative values"
        elif "empty" in issue_lower or "0 row" in issue_lower:
            return "Empty dataset"
        elif "duplicate" in issue_lower:
            return "Duplicates"
        elif "type" in issue_lower:
            return "Type mismatch"
        elif "date" in issue_lower:
            return "Invalid dates"
        else:
            return "Quality issue"
